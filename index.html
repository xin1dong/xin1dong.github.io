<!doctype html>
<html lang="en">
<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <!--  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">-->
  <meta name="viewport" content="width=1024">

  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="style.css">

  <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
          integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n"
          crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
          integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
          crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"
          integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6"
          crossorigin="anonymous"></script>

  <!-- Place this tag in your head or just before your close body tag. -->
  <script async defer src="https://buttons.github.io/buttons.js"></script>

  <title>Xin Dong</title>
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">
  <link rel="icon" href="images/favicon.ico" type="image/x-icon">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NBWSV8NZ21"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-NBWSV8NZ21');
</script>

<body>
<div class="main">
  <div class="container"></div>

  <div class="name">
    <h1>Xin Dong</h1>
    <p><a href="index.html">Home</a> | <a href="blogs/index.html">Blog</a></p>
  </div>

  <div class="header">

    <table class="table-header">
      <tbody>
      <tr>
        <td align="top">
          <img class="me" src="images/me.jpg" alt="Xin Dong" style="width:220px;">
          <p style="font-size: x-small">Cambridge, MA, May 2020</p>
        </td>
        <td align="left">
          <p>I am a research scientist at <a target="_blank" href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>.
            I received Ph.D. in Computer Science from <a target="_blank" href="https://www.seas.harvard.edu/">Harvard University</a> in 2023, 
            advised by <a target="_blank" href="https://www.eecs.harvard.edu/htk/">H.T. Kung</a>. 
            I have general research interests in deep learning, with a focus on designing accurate, 
            efficient and trustworthy systems for autonomous machines, LLM and GenAI.</p>

          <p>Prior to Harvard, I was a research assistant at Nanyang Technological University and UC San Diego. 
            I am a recipient of the Harvard James Mills Peirce Fellowship.
          </p>

          <p>
            <a href="https://scholar.google.com/citations?user=O8nBN64AAAAJ&hl=en"><img
                src="./images/google_scholar_icon.png" alt="google scholar" class="icon"></a>
            <a href="https://github.com/xindongol"><img src="./images/github_icon.png" alt="github" class="icon"></a>
            <a href="https://twitter.com/SimonXinDong"><img src="./images/twitter_icon.png" alt="twitter" class="icon"></a>
            <!-- <a href="cv/cv_jielei.pdf"><img src="./images/cv_icon.png" alt="cv" class="icon"></a> -->
          </p>
          <p><b>Email</b>: xind [at] nvidia.com, xindong [at] alumni.harvard.edu </p>
          <!-- <p style="color:red">I am actively looking for full-time Research Scientist/Engineer positions. </p> -->
          <!-- <p style="color:rgba(235, 29, 63, 0.999)">Research intern positions (2025) at NVIDIA available focused on foundation models. 
            Please drop your resume to {firstname}d@nvidia.com if interested.
          </p> -->
        </td>
      </tr>
      </tbody>
    </table>
  </div>

  <hr>

  <div class="news">
    <h2>News</h2>
    <ul>
      <li><span>Jun 2025</span> &raquo; We have released <a target="_blank" href="https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B">Nemotron-Research-Reasoning-Qwen-1.5B (Top10 trend)</a>, trained using ‚ÄúProlonged RL,‚Äù where we significantly scaled up RL training steps (+2k) and problems (+130k). The RL-trained model makes substantial progress on problems that the base model cannot solve. Our 1.5B model has achieved impressive results, on par with DeepSeek-R1-Distill-Qwen-7B.
      </li>
      <li><span>Apr 2025</span> &raquo; We have released CLIMB, a robust LLM pre- and post-training dataset building method.<br>
          We have now completed the puzzle of <span style="color:#3498db; font-size:1.05em">data preparation</span>, <span style="color:#e74c3c; font-size:1.05em">model architecture</span>, <span style="color:#2ecc71; font-size:1.05em">training recipes</span>, and <span style="color:#9b59b6; font-size:1.05em">alignment</span> to achieve state-of-the-art SLM.
          A small but strong reasoner is on the way.
      </li>
      <li><span>Jan 2025</span> &raquo; Gave a talk at <a href="https://www.csail.mit.edu/event/scale-ml-mlsys-reading-group-hymba-hybrid-head-architecture-small-language-models">Scale ML + MLSys @ MIT</a> on our Hymba work. Thanks for the invite.</li> 
      <li><span>Dec 2024</span> &raquo; üèÜ Join our <a href="https://sites.google.com/view/datafilteringchallenge/home?authuser=0">Data Filtering Challenge</a> for Edge LLMs and help shape the future of language models. Solve real-world problems, showcase your skills, and win amazing prizes! </li> 
      <li><span>Nov 2024</span> &raquo; We released the first hybrid-head model, <a href="https://huggingface.co/collections/nvidia/hymba-673c35516c12c4b98b5e845f">Hymba-1.5B</a> (accepted by ICLR 2025 as spotlight), which outperforms LLaMA 3.2-3B, despite being trained on 7√ó fewer tokens and achieving 12√ó cache reduction. Try them from Huggingface for your on-device LLM applications.&#128293;</li> 
      <!-- <li><span>Jan 2024</span> &raquo; Serve as Editor, Area Chair and Reviewer for ICLR, NeurIPS, ACM MM, and Electronics. </li>  -->
      <li><span>Oct 2022</span> &raquo; Our <a href="https://openreview.net/pdf?id=BkgXT24tDS">Additive Power-of-Two Quantization (ICLR'20)</a> is now supported by <a href="https://github.com/pytorch/pytorch/blob/dd516d7098122b3cb8a49ee7d23fdd8d214da213/torch/ao/quantization/experimental/quantizer.py#L8">offical PyTorch APIs</a>. It is a non-uniform quantization that fits well to weights distribution 
        and offers great hardware efficiency. Try it out!</li> 
      <li><span>Oct 2022</span> &raquo; Our <a href="https://arxiv.org/pdf/2107.06304.pdf">Direct Model Inversion</a> is accepted by BMVC 2022 and featured by <a href="https://www.technologyreview.com/2021/10/12/1036844/ai-gan-fake-faces-data-privacy-security-leak/">MIT Technology Review</a>, 
        <a href="https://singularityhub.com/2021/10/25/not-so-mysterious-after-all-researchers-show-how-to-crack-ais-black-box/">SingularityHub</a>. Thank collaborators from NVIDIA and Harvard.</li> 
      <!-- <li><span>Jul 2022</span> &raquo; Our paper on federated learning is accepted by ECCV 2022. Thank collaborators from Harvard and Deepmind.</li>
      <li><span>Mar 2022</span> &raquo; The Co-organized workshop on 
      The Practical Deep Learning in the Wild (<a target="_blank" href="https://practical-dl.github.io/">PracticalDL-22</a>) at AAAI 2022 is online now!</li>
      <li><span>Mar 2022</span> &raquo; Two first-author papers are accepted by CVPR 2022. Thanks collaborators from Meta, Deepmind, Harvard and UTD.</li> -->
    </ul>
  </div>


    <!-- experiments -->
    <!-- <div class="news">
      <h2>Selected Research Topics</h2>
  
      <div class="row my-3">
        <div class="col-lg-12">
          <div class="paper-text">
            <div class="paper-title">
              Distributed Training and Inference for DL (e.g., Federated Learning, Split Computing)
            </div>
            <div class="paper-conference">
              <a href="https://arxiv.org/abs/2207.09413">[ECCV'22]</a> <a href="https://arxiv.org/abs/2204.04705">[CVPR'22]</a>
            </div>
          </div>
        </div>
      </div>

      <div class="row my-3">
        <div class="col-lg-12">
          <div class="paper-text">
            <div class="paper-title">
              Trustworthy and Private DL (e.g., Out-of-Distribution/Adversarial Samples, Data Privacy)
            </div>
            <div class="paper-conference">
              <a href="https://arxiv.org/pdf/2104.11408.pdf">[CVPR'22]</a> <a href="https://arxiv.org/abs/2107.06304">[BMVC'22]</a>
            </div>
          </div>
        </div>
      </div>      

      <div class="row my-3">
        <div class="col-lg-12">
          <div class="paper-text">
            <div class="paper-title">
              Data and Computation Efficient Training and Inference (e.g., Pruning, Quantization) for DL Applications (e.g., CV, NLP)
            </div>
            <div class="paper-conference">
              <a href="https://arxiv.org/pdf/2104.11408.pdf">[CVPR'22]</a> <a href="https://dl.acm.org/doi/abs/10.1145/3445814.3446741">[ASPLOS'21]</a> <a href="http://proceedings.mlr.press/v139/li21d/li21d.pdf">[ICML'21]</a> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Li_MixMix_All_You_Need_for_Data-Free_Compression_Are_Feature_and_ICCV_2021_paper.pdf">[CVPR'21]</a> <a href="https://arxiv.org/pdf/1909.13144.pdf">[ICLR'20] (highly cited)</a> <a href="https://arxiv.org/pdf/1912.02057.pdf">[AAAI'20]</a> <a href="https://aclanthology.org/2020.findings-emnlp.129.pdf">[EMNLP'20]</a> <a href="https://arxiv.org/pdf/2003.07577.pdf">[ICLR'20]</a> <a href="https://arxiv.org/pdf/1812.04210.pdf">[CVPR'19]</a> <a href="https://arxiv.org/abs/1806.07550">[CVPR'19] (highly cited)</a> <a href="https://arxiv.org/pdf/1905.00462.pdf">[ICS'19]</a> <a href="http://www.eecs.harvard.edu/~htk/publication/2019-asap-kung-mcdanel-zhang-dong-chen.pdf">[ASAP'19]</a> <a href="https://papers.nips.cc/paper/2017/file/c5dc3e08849bec07e33ca353de62ea04-Paper.pdf">[NeurIPS'17] (highly cited)</a> 
            </div> 
          </div>
        </div>
      </div>

    </div> -->




  <!-- experiments -->
  <div class="projects">
    <h2>Experiences</h2>

    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper-center">
          <img class="paper-img" style="max-height:65px!important;max-width:65px!important;" src="images/nvidia.png">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            NVIDIA
          </div>
          <div class="paper-authors">
            Research Scientist, 2024 - Present
          </div>
        </div>
      </div>
    </div>

    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper-center">
          <img class="paper-img" style="max-height:65px!important;max-width:65px!important;" src="images/sony.jpg">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Sony
          </div>
          <div class="paper-authors">
            Research Scientist, 2023 - 2024
          </div>
        </div>
      </div>
    </div>

    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper-center">
          <img class="paper-img" style="max-height:65px!important;max-width:65px!important;" src="images/meta.jpg">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Meta Reality Lab
          </div>
          <div class="paper-authors">
            Research Scientist Intern, 2021, 2022
          </div>
        </div>
      </div>
    </div>

    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper-center">
          <img class="paper-img" style="max-height:65px!important;max-width:65px!important;" src="images/nvidia.png">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            NVIDIA
          </div>
          <div class="paper-authors">
            Research Scientist Intern, 2020
          </div>
        </div>
      </div>
    </div>


    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper-center">
          <img class="paper-img" style="max-height:65px!important;max-width:65px!important;" src="images/Tencent-Logo.jpg">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Tencent America
          </div>
          <div class="paper-authors">
            Research Scientist Intern, 2019
          </div>
        </div>
      </div>
    </div>


  </div>





  <div class="papers">
    <h2>Publications</h2>

    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" style="max-height:130px!important;max-width:195px!important;" src="images/hymba_mem.png" alt="gpt-pruning">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Hymba: A Hybrid-head Architecture for Small Language Models
          </div>
          <div class="paper-conference">International Conference on Learning Representations (<a href="https://iclr.cc/">ICLR 2025</a>)</div>
          <div class="paper-authors">
            <b>Xin Dong*</b>, Fonggan Fu*, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, Yingyan (Celine) Lin, Jan Kautz, Pavlo Molchanov
          </div>
          <div>
            <a target="_blank" href="https://arxiv.org/pdf/2411.13676">[PDF]</a>
            <a target="_blank" href="https://huggingface.co/collections/nvidia/hymba-673c35516c12c4b98b5e845f">[HFüî•]</a>
          </div>
        </div>
      </div>
    </div> 
    
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" style="max-height:130px!important;max-width:195px!important;" src="images/prorl.jpeg" alt="gpt-pruning">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models
          </div>
          <!-- <div class="paper-conference">International Conference on Learning Representations (<a href="https://iclr.cc/">ICLR 2025</a>)</div> -->
          <div class="paper-authors">
            Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, <b>Xin Dong</b>, Yejin Choi, Jan Kautz, Yi Dong
          </div>
          <div>
            <a target="_blank" href="https://arxiv.org/abs/2505.24864">[PDF]</a>
            <a target="_blank" href="https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B">[HFüî•]</a>
          </div>
        </div>
      </div>
    </div> 


    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" style="max-height:130px!important;max-width:195px!important;" src="images/climb.png" alt="gpt-pruning">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training
          </div>
          <!-- <div class="paper-conference">International Conference on Learning Representations (<a href="https://iclr.cc/">ICLR 2025</a>)</div> -->
          <div class="paper-authors">
            Shizhe Diao, Yu Yang, Yonggan Fu, <b>Xin Dong</b>, Dan Su, Markus Kliegl, Zijia Chen, Peter Belcak, Yoshi Suhara, Hongxu Yin, Mostofa Patwary, Yingyan (Celine) Lin, Jan Kautz, Pavlo Molchanov
          </div>
          <div>
            <a target="_blank" href="https://arxiv.org/abs/2504.13161">[PDF]</a>
            <a target="_blank" href="https://huggingface.co/collections/nvidia/climb-datasets-67e428bdb9aaced2acda191f">[Code]</a>
          </div>
        </div>
      </div>
    </div>      




    
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" style="max-height:130px!important;max-width:195px!important;" src="images/incontext.png" alt="gpt-pruning">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Unraveling the Mechanics of Learning-Based Demonstration Selection for In-Context Learning
          </div>
          <div class="paper-conference">Annual Meeting of the Association for Computational Linguistics (<a href="https://2025.aclweb.org/">ACL 2025</a>)</div>
          <div class="paper-authors">
            Hui Liu, Wenya Wang, Hao Sun, Chris Xing Tian, Chenqi Kong, <b>Xin Dong</b>, Haoliang Li
          </div>
          <div>
            <a target="_blank" href="https://arxiv.org/pdf/2406.11890">[PDF]</a>
            <!-- <a target="_blank" href="https://huggingface.co/collections/nvidia/climb-datasets-67e428bdb9aaced2acda191f">[Code]</a> -->
          </div>
        </div>
      </div>
    </div>     


    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" style="max-height:130px!important;max-width:195px!important;" src="images/longmamba.png" alt="gpt-pruning">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            LongMamba: Enhancing Mamba's Long-Context Capabilities via Training-Free Receptive Field Enlargement
          </div>
          <div class="paper-conference">International Conference on Learning Representations (<a href="https://iclr.cc/">ICLR 2025</a>)</div>
          <div class="paper-authors">
            Zhifan Ye, Kejing Xia, Yonggan Fu, <b>Xin Dong</b>, Jihoon Hong, Xiangchi Yuan, Shizhe Diao, Jan Kautz, Pavlo Molchanov, Yingyan Celine Lin
          </div>
          <div>
            <a target="_blank" href="https://openreview.net/pdf?id=fMbLszVO1H">[PDF]</a>
            <!-- <a target="_blank" href="https://huggingface.co/collections/nvidia/hymba-673c35516c12c4b98b5e845f">[Code]</a> -->
          </div>
        </div>
      </div>
    </div> 

    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" style="max-height:130px!important;max-width:195px!important;" src="images/Lacache.png" alt="gpt-pruning">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            	LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models
          </div>
          <div class="paper-conference">International Conference on Machine Learning (<a href="https://icml.cc/">ICML 2025</a>)</div>
          <div class="paper-authors">
            Dachuan Shi, Yonggan Fu, Xiangchi Yuan, Zhongzhi Yu, Haoran You, Sixu Li, <b>Xin Dong</b>, Jan Kautz, Pavlo Molchanov, Yingyan Celine Lin
          </div>
          <div>
            <a target="_blank" href="https://www.simonxin.com">[PDF]</a>
            <!-- <a target="_blank" href="https://huggingface.co/collections/nvidia/hymba-673c35516c12c4b98b5e845f">[Code]</a> -->
          </div>
        </div>
      </div>
    </div>     

    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" style="max-height:130px!important;max-width:195px!important;" src="images/depth_pruning_llm.png" alt="gpt-pruning">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            A Deeper Look at Depth Pruning of LLMs
          </div>
          <div class="paper-conference">ICML 2024 Workshop on Theoretical Foundations of Foundation Models (<a href="https://sites.google.com/view/tf2m">ICML Workshop 2024</a>)</div>
          <div class="paper-authors">
            Shoaib Ahmed Siddiqui, <b>Xin Dong</b>, Greg Heinrich, Thomas Breuel, Jan Kautz, David Krueger, Pavlo Molchanov
          </div>
          <div>
            <a target="_blank" href="https://www.arxiv.org/abs/2407.16286">[PDF]</a>
            <a target="_blank" href="https://github.com/shoaibahmed/llm_depth_pruning">[Code]</a>
          </div>
        </div>
      </div>
    </div>    

    <!--gpt pruning-->
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" style="max-height:130px!important;max-width:195px!important;" src="images/gpt-pruning.png" alt="gpt-pruning">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning
          </div>
          <div class="paper-conference">International Conference on Learning Representations (<a href="https://iclr.cc/">ICLR 2024</a>)</div>
          <div class="paper-authors">
            Tian Jin*, Nolan Clement*, <b>Xin Dong*</b>, Vaishnavh Nagarajan, Michael Carbin, Jonathan Ragan-Kelley, Gintare Karolina Dziugaite
          </div>
          <div class="paper-authors">
            (<b>*: equal contribution</b>)
          </div> 
          <div>
            <a target="_blank" href="https://arxiv.org/pdf/2310.04680.pdf">[PDF]</a>
          </div>
        </div>
      </div>
    </div>


    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" style="max-height:130px!important;max-width:195px!important;" src="images/watermark_detect.png" alt="gpt-pruning">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Finding needles in a haystack: A Black-Box Approach to Invisible Watermark Detection
          </div>
          <div class="paper-conference">2024 European Conference on Computer Vision (<a href="https://eccv.ecva.net/">ECCV 2024</a>)</div>
          <div class="paper-authors">
            Minzhou Pan, Zhenting Wang, <b>Xin Dong</b>, Vikash Sehwag, Lingjuan Lyu, Xue Lin
          </div>
          <div>
            <a target="_blank" href="https://arxiv.org/abs/2403.15955">[PDF]</a>
            <!-- <a target="_blank" href="https://github.com/shoaibahmed/llm_depth_pruning">[Code]</a> -->
          </div>
        </div>
      </div>
    </div>  

    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" style="max-height:130px!important;max-width:195px!important;" src="images/background_od.png" alt="gpt-pruning">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            A Simple Background Augmentation Method for Object Detection with Diffusion Mode
          </div>
          <div class="paper-conference">2024 European Conference on Computer Vision (<a href="https://eccv.ecva.net/">ECCV 2024</a>)</div>
          <div class="paper-authors">
            Yuhang Li, <b>Xin Dong</b>, Chen Chen, Weiming Zhuang, Lingjuan Lyu
          </div>
          <div>
            <a target="_blank" href="">[PDF]</a>
            <!-- <a target="_blank" href="https://github.com/shoaibahmed/llm_depth_pruning">[Code]</a> -->
          </div>
        </div>
      </div>
    </div>  

    <!--gpt pruning-->
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" style="max-height:130px!important;max-width:195px!important;" src="images/ood_seg.png" alt="gpt-pruning">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Segment Every Out-of-Distribution Object
          </div>
          <div class="paper-conference">IEEE/CVF Conference on Computer Vision and Pattern Recognition (<a href="https://cvpr2024.thecvf.com/">CVPR 2024</a>)</div>
          <div class="paper-authors">
            Wenjie Zhao, Jia Li, <b>Xin Dong</b>, Yu Xiang, Yunhui Guo
          </div>
          <div>
            <a target="_blank" href="https://arxiv.org/abs/2311.16516">[PDF]</a>
            <a target="_blank" href="https://github.com/WenjieZhao1/S2M">[Code]</a>
          </div>
        </div>
      </div>
    </div>


    <!--fed-->
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" style="max-height:130px!important;max-width:195px!important;" src="images/fedicon.png" alt="fedicon">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Is Heterogeneity Notorious? Taming Heterogeneity to Handle Test-Time Shift in Federated Learning
          </div>
          <div class="paper-conference">Conference on Neural Information Processing Systems (<a href="https://nips.cc/virtual/2023/poster/70366">NeurIPS 2023</a>)</div>
          <div class="paper-authors">
            Yue Tan, Chen Chen, Weiming Zhuang, <b>Xin Dong</b>, Lingjuan Lyu, Guodong Long
          </div>
          <div>
            <a target="_blank" href="https://arxiv.org/pdf/2310.04680.pdf">[PDF]</a>
          </div>
        </div>
      </div>
    </div>    
    
    <!--mTVR-->
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" style="max-height:130px!important;max-width:195px!important;" src="images/dci.png" alt="mTVR">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Privacy Vulnerability of Split Computing to Data-Free Model Inversion Attacks
          </div>
          <div class="paper-conference">British Machine Vision Conference (<a href="https://bmvc2022.org/">BMVC 2022</a>)</div>
          <div class="paper-authors">
            <b>Xin Dong</b>, Hongxu Yin, Jose M. Alvarez, Jan Kautz, Pavlo Molchanov, H.T. Kung
          </div>
          <div>
            <a target="_blank" href="https://arxiv.org/abs/2107.06304">[PDF]</a>
          </div>
        </div>
      </div>
    </div>


    <!--mTVR-->
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" style="max-height:130px!important;max-width:160px!important;" src="images/HFL.png" alt="mTVR">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            SphereFed: Hyperspherical Federated Learning
          </div>
          <div class="paper-conference">European Conference on Computer Vision (<a href="https://eccv2022.ecva.net/">ECCV 2022</a>)</div>
          <div class="paper-authors">
            <b>Xin Dong</b>, Sai Qian Zhang, Ang Li, H.T. Kung
          </div>
          <div>
            <a target="_blank" href="https://arxiv.org/abs/2207.09413">[PDF]</a>
          </div>
        </div>
      </div>
    </div>

    

    <!--mTVR-->
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" style="max-height:130px!important;max-width:250px!important;" src="images/ARVR_system.png" alt="mTVR">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            SplitNets: Designing Neural Architectures for Efficient Distributed Computing on Head-Mounted Systems
          </div>
          <div class="paper-conference">IEEE/CVF Conference on Computer Vision and Pattern Recognition (<a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>)</div>
          <div class="paper-authors">
            <b>Xin Dong</b>, Barbara De Salvo, Meng Li, Chiao Liu, Zhongnan Qu, H.T. Kung, Ziyun Li
          </div>
          <div>
            <a target="_blank" href="https://arxiv.org/abs/2204.04705">[PDF]</a>
          </div>
        </div>
      </div>
    </div>



    <!--VL-T5-->
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" src="images/bn_ood.png" alt="VL-T5">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Neural Mean Discrepancy for Efficient Out-of-Distribution Detection
          </div>
          <div class="paper-conference">IEEE/CVF Conference on Computer Vision and Pattern Recognition (<a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>)</div>
          <div class="paper-authors">
            <b>Xin Dong</b>, Junfeng Guo, Ang Li, Wei-Te Ting, Cong Liu, H.T. Kung
          </div>
          <div>
            <a target="_blank" href="https://arxiv.org/pdf/2104.11408.pdf">[PDF]</a>
          </div>
        </div>
      </div>
    </div>


    <!--VL-T5-->
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" src="images/dress.png" alt="VL-T5">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            DRESS: Dynamic REal-time Sparse Subnets
          </div>
          <div class="paper-conference">Efficient Deep Learning for Computer Vision (ECV) CVPR Workshop (<a href="https://cvpr2022.thecvf.com/">ECV 2022</a>)</div>
          <div class="paper-authors">
            Zhongnan Qu, Syed Shakib Sarwar, <b>Xin Dong</b>, Yuecheng Li, Ekin Sumbul, Barbara De Salvo
          </div>
          <div>
            <a target="_blank" href="https://arxiv.org/abs/2207.00670">[PDF]</a>
          </div>
        </div>
      </div>
    </div>

    <!--ClipBERT-->
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" src="images/term_quant.png" alt="VL-T5">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Training for Multi-resolution Inference Using Reusable Quantization Terms
            <!-- <span class="text-red">(Oral)</span> -->
          </div>
          <div class="paper-conference">The 26th ACM International Conference on Architectural 
            Support for Programming Languages and Operating Systems (<a href="https://asplos-conference.org/2021/">ASPLOS 2021</a>)</div>
          <div class="paper-authors">
            Sai Qian Zhang, Bradley McDanel, H.T. Kung, <b>Xin Dong</b>
          </div>
          <div>
            <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3445814.3446741">[PDF]</a>
            <a target="_blank" href="https://github.com/saizhang0218/Multi-resolution-Inference">[Code]</a>
<!--             <a class="github-button" href="https://github.com/jayleicn/ClipBERT" data-icon="octicon-star"
               data-show-count="true" aria-label="Star jayleicn/ClipBERT on GitHub">Star</a> -->
          </div>
        </div>
      </div>
    </div>



    <!--ClipBERT-->
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" style="max-height:130px!important;max-width:150px!important;" src="images/snn_freelunch.png" alt="VL-T5">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            A free lunch from ANN: Towards efficient, accurate spiking neural networks calibration
            <!-- <span class="text-red">(Oral)</span> -->
          </div>
          <div class="paper-conference">International Conference on Machine Learning (<a href="https://icml.cc/Conferences/2021">ICML 2021</a>)</div>
          <div class="paper-authors">
            Yuhang Li, Shikuang Deng, <b>Xin Dong</b>, Ruihao Gong, Shi Gu
          </div>
          <div>
            <a target="_blank" href="http://proceedings.mlr.press/v139/li21d/li21d.pdf">[PDF]</a>
            <a target="_blank" href="https://github.com/yhhhli/SNN_Calibration">[Code]</a>
<!--             <a class="github-button" href="https://github.com/jayleicn/ClipBERT" data-icon="octicon-star"
               data-show-count="true" aria-label="Star jayleicn/ClipBERT on GitHub">Star</a> -->
          </div>
        </div>
      </div>
    </div>


    <!--ClipBERT-->
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" src="images/mixmix.png" alt="VL-T5">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing
            <!-- <span class="text-red">(Oral)</span> -->
          </div>
          <div class="paper-conference">IEEE/CVF Conference on Computer Vision and Pattern Recognition (<a href="https://cvpr2021.thecvf.com/">CVPR 2021</a>)</div>
          <div class="paper-authors">
            Yuhang Li, Feng Zhu, Ruihao Gong, Mingzhu Shen, <b>Xin Dong</b>, Fengwei Yu, Shaoqing Lu, Shi Gu
          </div>
          <div>
            <a target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Li_MixMix_All_You_Need_for_Data-Free_Compression_Are_Feature_and_ICCV_2021_paper.pdf">[PDF]</a>
            <!-- <a target="_blank" href="https://github.com/yhhhli/SNN_Calibration">[Code]</a> -->
<!--             <a class="github-button" href="https://github.com/jayleicn/ClipBERT" data-icon="octicon-star"
               data-show-count="true" aria-label="Star jayleicn/ClipBERT on GitHub">Star</a> -->
          </div>
        </div>
      </div>
    </div>    




    <!--MART-->
    <div class="row mb-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" src="images/apot.png" alt="MART">
        </div>
      </div>

      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
             Additive Powers-of-Two Quantization: A Non-uniform Discretization for Neural Networks
          </div>
          <div class="paper-conference">International Conference on Learning Representations (<a href="https://acl2020.org/">ICLR 2020</a>)</div>
          <div class="paper-authors">
            Yuhang Li*, <b>Xin Dong*</b>, Wei Wang 
          </div>
           <div class="paper-authors">
            (<b>*: equal contribution</b>)
          </div>        
          <div>
            <a href="https://arxiv.org/pdf/1909.13144.pdf">[PDF]</a>
            <!-- <a href="./files/acl20_mart_talk.pptx">[Slides]</a> -->
            <a href="https://github.com/yhhhli/APoT_Quantization">[Code]</a>
            <a class="github-button" href="https://github.com/yhhhli/APoT_Quantization"
               data-icon="octicon-star" data-show-count="true"
               aria-label="Star APoT_Quantization on GitHub">Star</a>

          </div>
        </div>
      </div>
    </div>



    <!--TVQA PLUS-->
    <div class="row mb-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" src="images/rtn.png" alt="TVQA PLUS">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            RTN: Reparameterized Ternary Network
          </div>
          <div class="paper-conference">The Thirty-Fourth AAAI Conference on Artificial Intelligence (<a href="https://aaai.org/Conferences/AAAI-20/">AAAI 2020</a>)</div>
          <div class="paper-authors">
            Yuhang Li*, <b>Xin Dong*</b>, Sai Qian Zhang, HaoliBai, Yuanpeng Chen, Wei Wang 
          </div>
          <div class="paper-authors">
            (<b>*: equal contribution</b>)
          </div>
          <div>
            <a href="https://arxiv.org/pdf/1912.02057.pdf">[PDF]</a>
<!--             <a href="./files/acl20_tvqa_plus_talk.pptx">[Slides]</a>
            <a target="_blank" href="http://tvqa.cs.unc.edu/">[Dataset]</a>
            <a target="_blank" href="https://github.com/jayleicn/TVQA-PLUS">[Code]</a>
            <a class="github-button" href="https://github.com/jayleicn/TVQAplus"
               data-icon="octicon-star" data-show-count="true"
               aria-label="Star jayleicn/recurrent-transformer on GitHub">Star</a> -->
          </div>
        </div>
      </div>
    </div>



    <!--VLEP-->
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" src="images/exbert.png" alt="VLEP">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            exBERT: Extending Pre-trained Models with Domain-specific Vocabulary Under Constrained Training Resources
          </div>
          <div class="paper-conference">The 2020 Conference on Empirical Methods in Natural Language Processing (<a href="https://2020.emnlp.org/">EMNLP 2020</a>)</div>
          <div class="paper-authors">
            Wen Tai, H.T. Kung, <b>Xin Dong</b>, Marcus Comiter, Chang-Fu Kuo
          </div>
          <div>
            <a target="_blank" href="http://www.eecs.harvard.edu/~htk/publication/2020-emnlp-tai-kung-dong-comiter-kuo.pdf">[PDF]</a>
            <a target="_blank" href="#"></a>
            <a target="_blank" href="https://github.com/cgmhaicenter/exBERT">[Code]</a>
<!--            <a class="github-button" href="https://github.com/jayleicn/VideoLanguageFuturePred" data-icon="octicon-star"-->
<!--               data-show-count="true" aria-label="Star jayleicn/VideoLanguageFuturePred on GitHub">Star</a>-->
          </div>
        </div>
      </div>
    </div>

    <!--TVR-->
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" src="images/ebs.png" alt="TVR">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Efficient Bitwidth Search for Practical Mixed Precision Neural Network
          </div>
          <div class="paper-conference"><a href="https://arxiv.org/">arXiv 2020</a></div>
          <div class="paper-authors">
            Yuhang Li, Wei Wang, Haoli Bai, Ruihao Gong, <b>Xin Dong</b>, Fengwei Yu
          </div>
          <div>
            <a target="_blank" href="https://arxiv.org/pdf/2003.07577.pdf">[PDF]</a>
          </div>
        </div>
      </div>
    </div>


    <!--TVR-->
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" src="images/nas_binary.png" alt="TVR">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Differentiable Dimension Search for Binary Neural Networks
          </div>
          <div class="paper-conference">1st Workshop on Neural Architecture Search at ICLR 2020 <a href="https://sites.google.com/view/nas2020/accepted-papers">(ICLR 2020 Workshops))</a></div>
          <div class="paper-authors">
            Yuhang Li, Ruihao Gong, Fengwei Yu, <b>Xin Dong</b>, Xianglong Liu
          </div>
          <div>
            <a target="_blank" href="https://xhplus.github.io/publication/2020-iclrw-dms/2020-iclrw-dms.pdf">[PDF]</a>
          </div>
        </div>
      </div>
    </div>





    <!--TVQA-->
    <div class="row mb-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" src="images/main_sub_binary.png" alt="TVQA">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            A Main/Subsidiary Network Framework for Simplifying Binary Neural Network
            <!-- <span class="text-red">(Oral)</span> -->
          </div>
          <div class="paper-conference">IEEE/CVF Conference on Computer Vision and Pattern Recognition (<a href="https://cvpr2019.thecvf.com/">CVPR 2019</a>)</div>
          <div class="paper-authors">
            Yinghao Xu*, <b>Xin Dong*</b>, Yudian Li, Hao Su      
          </div>
          <div class="paper-authors">
            (<b>*: equal contribution</b>)
          </div>          
          <div>
            <a target="_blank" href="https://arxiv.org/pdf/1812.04210.pdf">[PDF]</a>
<!--             <a href="./files/tvqa_emnlp18_oral.pptx">[Slides]</a>
            <a target="_blank" href="http://tvqa.cs.unc.edu/">[Dataset]</a> -->
            <a target="_blank" href="https://github.com/justimyhxu/MainSubsidaryBNN">[Code]</a>
            <!--            <iframe src="https://ghbtns.com/github-btn.html?user=jayleicn&repo=TVQA&type=star&count=true&v=2"-->
            <!--                    frameborder="0" scrolling="0" width="100px" height="20px" align="top" style="margin-top:4px"></iframe>-->
<!--             <a class="github-button" href="https://github.com/jayleicn/TVQA" data-icon="octicon-star"
               data-show-count="true" aria-label="Star jayleicn/TVQA on GitHub">Star</a> -->
          </div>
        </div>
      </div>

    </div>


    
        <!--TVQA-->
    <div class="row mb-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" src="images/BENN.png" alt="benn">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?
            <!-- <span class="text-red">(Oral)</span> -->
          </div>
          <div class="paper-conference">IEEE/CVF Conference on Computer Vision and Pattern Recognition (<a href="https://cvpr2019.thecvf.com/">CVPR 2019</a>)</div>
          <div class="paper-authors">
            Shilin Zhu, <b>Xin Dong</b>, Hao Su
          </div>
          <div>
            <a target="_blank" href="https://arxiv.org/abs/1806.07550">[PDF]</a>
<!--             <a href="./files/tvqa_emnlp18_oral.pptx">[Slides]</a>
            <a target="_blank" href="http://tvqa.cs.unc.edu/">[Dataset]</a> -->
            <a target="_blank" href="https://github.com/XinDongol/BENN-PyTorch">[Code]</a>
            <!--            <iframe src="https://ghbtns.com/github-btn.html?user=jayleicn&repo=TVQA&type=star&count=true&v=2"-->
            <!--                    frameborder="0" scrolling="0" width="100px" height="20px" align="top" style="margin-top:4px"></iframe>-->
<!--             <a class="github-button" href="https://github.com/jayleicn/TVQA" data-icon="octicon-star"
               data-show-count="true" aria-label="Star jayleicn/TVQA on GitHub">Star</a> -->
          </div>
        </div>
      </div>

    </div>



    <!--CRV classification-->
    <div class="row mb-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" src="images/full_stack.png" alt="image classification">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Full-stack Optimization for Accelerating CNNs with FPGA Validation
          </div>
          <div class="paper-conference">The ACM International Conference on Supercomputing (<a href="https://ics19.eecis.udel.edu/">ICS 2019</a>)
          </div>
          <div class="paper-authors">
            Bradley McDanel, Sai Qian Zhang, H.T. Kung, <b>Xin Dong</b>
          </div>
          <div>
            <a target="_blank" href="https://arxiv.org/pdf/1905.00462.pdf">[PDF]</a>
<!--             <a target="_blank" href="https://github.com/jayleicn/classification-with-coarse-fine-labels">[Code]</a>
            <a class="github-button" href="https://github.com/jayleicn/classification-with-coarse-fine-labels" data-icon="octicon-star"
               data-show-count="true" aria-label="Star jayleicn/classification-with-coarse-fine-labels on GitHub">Star</a> -->
          </div>
        </div>
      </div>
    </div>




<!--CRV classification-->
    <div class="row mb-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" src="images/maestro.png" alt="image classification">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Maestro: A Memory-on-Logic Architecture for Coordinated Parallel Use of Many Systolic Arrays
          </div>
          <div class="paper-conference">The 30th IEEE International Conference on 
            Application-specific Systems, Architectures and Processors (<a href="https://asap2019.csl.cornell.edu/">ASAP 2019</a>)
          </div>
          <div class="paper-authors">
            Bradley McDanel, Sai Qian Zhang, H.T. Kung, <b>Xin Dong</b>
          </div>
          <div>
            <a target="_blank" href="http://www.eecs.harvard.edu/~htk/publication/2019-asap-kung-mcdanel-zhang-dong-chen.pdf">[PDF]</a>
<!--             <a target="_blank" href="https://github.com/jayleicn/classification-with-coarse-fine-labels">[Code]</a>
            <a class="github-button" href="https://github.com/jayleicn/classification-with-coarse-fine-labels" data-icon="octicon-star"
               data-show-count="true" aria-label="Star jayleicn/classification-with-coarse-fine-labels on GitHub">Star</a> -->
            <a target="_blank" href="https://asap2019.csl.cornell.edu/presentations/9_McDanel.pdf">[Slides]</a>
          </div>
        </div>
      </div>
    </div>




<!--CRV classification-->
    <div class="row mb-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" src="images/lobs.png" alt="image classification">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon
          </div>
          <div class="paper-conference">Thirty-first Conference on Neural Information Processing Systems (<a href="https://nips.cc/Conferences/2017">NeurIPS 2017</a>)
          </div>
          <div class="paper-authors">
            <b>Xin Dong</b>, Shangyu Chen, Sinno Jialin Pan
          </div>
          <div>
            <a target="_blank" href="https://papers.nips.cc/paper/2017/file/c5dc3e08849bec07e33ca353de62ea04-Paper.pdf">[PDF]</a>
            <a target="_blank" href="https://github.com/csyhhu/L-OBS">[Code]</a>
<!--             <a class="github-button" href="https://github.com/jayleicn/classification-with-coarse-fine-labels" data-icon="octicon-star"
               data-show-count="true" aria-label="Star jayleicn/classification-with-coarse-fine-labels on GitHub">Star</a> -->
          </div>
        </div>
      </div>
    </div>



  </div>


<!--   <div class="projects">
    <h2>Services</h2>
    <div class="row my-3">
      <div class="col-lg-3">
        <div class="paper-img-wrapper">
          <img class="paper-img" src="images/anime.png">
        </div>
      </div>
      <div class="col-lg-9">
        <div class="paper-text">
          <div class="paper-title">
            AnimeGAN: Create Anime Face using Generative Adversarial Networks
          </div>
          <div class="paper-authors">
            <b>Jie Lei</b>
          </div>
          <div class="paper-conference">
            A simple GAN model that could automatically generate anime girl faces.
          </div>
          <div style="vertical-align: bottom!important;">
            <a target="_blank" href="https://github.com/jayleicn/animeGAN">[Code & AnimeFace Dataset]</a>
            <a class="github-button" href="https://github.com/jayleicn/animeGAN" data-icon="octicon-star"
               data-show-count="true" aria-label="Star jayleicn/animeGAN on GitHub">Star</a>
          </div>
        </div>
      </div>
    </div>
  </div> -->

  <!--<hr>-->

  <div class="misc">
    <h2>Academic Services</h2>
    <ul>
      <li>Reviewer/Area Chair for ICML, NeurIPS, AAAI, IJCAI, CVPR, ICCV, ECCV, EMNLP, ACL</li>
      <li>Co-organizer of the 1st international workshop on 
        The Practical Deep Learning in the Wild (<a target="_blank" href="https://practical-dl.github.io/">PracticalDL-22</a>) at AAAI 2022</li>
      <li>Teaching Fellow of Harvard CS242 <a target="_blank" href="https://www.seas.harvard.edu/computer-science/courses">Compute at Scale</a></li>
    </ul>
  </div>


</div>
</body>
</html>
