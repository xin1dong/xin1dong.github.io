# From Giants to Geniuses: The Rise of Small Agentic Models


Published on May 18, 2025 | Author: Xin Dong

---

Scaling law [[Kaplan et al., 2020](https://arxiv.org/abs/2001.08361), [Hoffmann et al., 2022](https://arxiv.org/abs/2203.15556)] for language (and multi-modal) models may be the most important discovery in the past decade for AI. 
The scaling law states that the performance of a language model is proportional to the number of parameters in the model when you scale the training compute and data accordingly. 
In order to push the intelligence boundary, the community, including many well-known companies and institutions, have been working on building larger and larger models. Taking the open-weight models as an example, we have seen from `Llama 1` to `Llama 4` that the model size has increased from 65B [[Touvron et al., 2023](https://arxiv.org/abs/2302.13971)] to 2,000B [[Meta AI, 2025](https://www.llama.com/models/llama-4/)] parameters. 

So, two questions naturally come to mind:
> 1. Is there a wall for the scaling law and when will it hit the wall?
> 2. What about the opposite direction, where small language models are entering their own Cambrian explosion era?

For the first question, Ilya Sutskever's talk at NeurIPS 2024 ["Pre-training as we know it will end"](https://www.youtube.com/watch?v=1yvBqasHLZs&t=6s) offered an intriguing analogy: Internet data is the fossil fuel of AI. Fortunately, we discovered this resource and leveraged it to build large language models (LLMs). Unfortunately, we have only one Internet. In the second era of LLMs, reinforcement learning has illuminated a new path where models can be trained with experience and exploratory data generated by the models themselves [[DeepSeek-AI, 2025](https://arxiv.org/abs/2501.12948), [OpenAI o3, 2025](https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf)]. 

In this blog, we will focus on the second question where we show early signs and evidence for the Cambrian explosion of small language models. 

## Why Small Language Models?

The reasons why small language models are becoming more and more popular are quite straightforward: they are lightweight, fast, and economical. 

### Realtime

Building agents that can be integrated into real-world applications usually requires the model behind the agent to respond in low latency. Otherwise, the user experience and the agent's use scenario will be severely impacted. In addition, the test-time scaling (i.e., the long chain-of-thought) make this even more challenging. For example, DeepSeek-R1 achieves ~40ms per token on average with 8xNVIDIA H200 setup, using well-optimized SGLang inference engine [[DataCrunch, 2025](https://datacrunch.io/blog/deploy-deepseek-r1-on-8x-nvidia-h200?utm_source=chatgpt.com)]. Suppose 2000 tokens are required for an agent to understand what the next function call should be, the latency will be 80 seconds. 
If we can use a smaller language model, the latency can be reduced to within 1 second and make the agent more responsive. 

### Economic Inference 

Apparently, large language models suffer from not only high response latency but also substantial inference costs. As mentioned above, deploying the DeepSeek-R1 (671B) requires a cluster of eight NVIDIA H200 GPUs, costing approximately $60,000 per month. 

If the model is small enough to be deployed on a single (and consumer-grade) GPU, the inference cost and deployment cost will be significantly reduced for both the model provider and the users. In addition, this can also be a good way to achieve the decentralized inference and "AI for all". 

### Cost of Updating

Small language models are also more cost-effective to update. As agentic data continues to grow and new information becomes essential, models require frequent updates to remain relevant. For example, a coding assistant model needs updating whenever new developer documentation is released or when significant bugs are identified and fixed. 

### Models Router

Small language model can be used with large language models to enjoy the benefits of both. A model router can be used to dispatch the query to the appropriate model based on the query types, query diffculties, and model capabilities. In addition, small language models can also be used with other special-purpose (small) models in an Agent2Agent framework [[Google, 2025](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/)].

<!-- ### Acdemic Pancipation -->




## David vs. Goliath: Can Small Language Models Rise to the Challenge of Agentic Intelligence?

In the past few months, we have seen a series of works that small models are able to outperform their (last-generation) larger counterparts in various tasks. 

### Language Models

<table style="border-collapse: collapse; width: 100%; margin: 20px 0; font-size: 0.9em; font-family: inherit;">
  <tr style="background-color: #f2f2f2;">
    <th style="border: 1px solid #ddd; padding: 6px; text-align: center; font-weight: bold; font-size: 0.9em; width: 20%;">Model</th>
    <th style="border: 1px solid #ddd; padding: 6px; text-align: center; font-weight: bold; font-size: 0.9em; width: 15%;">Size</th>
    <th style="border: 1px solid #ddd; padding: 6px; text-align: center; font-weight: bold; font-size: 0.9em; width: 65%;">Performance</th>
  </tr>
  <tr>
    <td style="border: 1px solid #ddd; padding: 6px; text-align: center; font-size: 0.9em;"><a href="https://huggingface.co/meta-llama/Llama-3.2-3B">Llama-3.2</a></td>
    <td style="border: 1px solid #ddd; padding: 6px; text-align: center; font-size: 0.9em;">1B, 3B</td>
    <td style="border: 1px solid #ddd; padding: 6px; text-align: left; font-size: 0.9em;">Llama 3.2 is a collection of multilingual large language models. Comparing to GPT-3 (175B), Llama 3.2 3B achieve better MMLU score (58% vs 43.9%).</td>
  </tr>
  <tr style="background-color: #f9f9f9;">
    <td style="border: 1px solid #ddd; padding: 6px; text-align: center; font-size: 0.9em;">Qwen-2.5 and -3</td>
    <td style="border: 1px solid #ddd; padding: 6px; text-align: center; font-size: 0.9em;">0.5B, 1.5B, 3B</td>
    <td style="border: 1px solid #ddd; padding: 6px; text-align: left; font-size: 0.9em;">Qwen serial small language models show stronger benchmark scores than Llama 3.2 and are widely used in the open-source community. It has great instruction following, structured output, role-playing, and reasoning capabilities. </td>
  </tr>
  <tr>
    <td style="border: 1px solid #ddd; padding: 6px; text-align: center; font-size: 0.9em;">Claude 3</td>
    <td style="border: 1px solid #ddd; padding: 6px; text-align: center; font-size: 0.9em;">140B</td>
    <td style="border: 1px solid #ddd; padding: 6px; text-align: left; font-size: 0.9em;">95% performance rating overall</td>
  </tr>
  <tr style="background-color: #f9f9f9;">
    <td style="border: 1px solid #ddd; padding: 6px; text-align: center; font-size: 0.9em;">GPT-4o</td>
    <td style="border: 1px solid #ddd; padding: 6px; text-align: center; font-size: 0.9em;">220B</td>
    <td style="border: 1px solid #ddd; padding: 6px; text-align: left; font-size: 0.9em;">97% on standard ML benchmarks</td>
  </tr>
  <tr>
    <td style="border: 1px solid #ddd; padding: 6px; text-align: center; font-size: 0.9em;">Gemini Pro</td>
    <td style="border: 1px solid #ddd; padding: 6px; text-align: center; font-size: 0.9em;">120B</td>
    <td style="border: 1px solid #ddd; padding: 6px; text-align: left; font-size: 0.9em;">94% average across multiple test suites</td>
  </tr>
</table>

### Speech Models 

### 


## How to Build (Better) Small Language Models?


## Conclusion

